{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "33b12567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import lossmaps as lm\n",
    "import xobjects as xo\n",
    "\n",
    "import xtrack as xt\n",
    "import xcoll as xc\n",
    "\n",
    "import lossmaps as lm\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import json\n",
    "\n",
    "import xtrack as xt\n",
    "import xpart as xp\n",
    "import xobjects as xo\n",
    "\n",
    "\n",
    "import pickle \n",
    "import h5py\n",
    "import io\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef58df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineData():\n",
    "    \n",
    "    def __init__(self, run, # 'HL' or 'Run3'\n",
    "                 line_file_name, # = 'config_sim.yaml',\n",
    "                 coll_file_name, # = 'config_sim.yaml'\n",
    "                 TCCS_name = 'tccs.5r3.b2',\n",
    "                 TCCP_name = 'tccp.4l3.b2',\n",
    "                 TARGET_name = 'target.4l3.b2',\n",
    "                 TCCS_loc_b1 =  6773.7,\n",
    "                 TCCP_loc_b1 = 6653.3,\n",
    "                 beam = 2, plane = 'V', engine = 'everest', sigma_TCCS = None, sigma_TCCP = None,\n",
    "                 job_num_part = 100000, job_num_turns = 200, \n",
    "              ):\n",
    "\n",
    "        self.run = run\n",
    "        self.line_file = f\"{os.environ.get('HOME_TWOCRYST')}/{line_file_name}\"\n",
    "        self.coll_file = f\"{os.environ.get('HOME_TWOCRYST')}/{coll_file_name}\"\n",
    "        \n",
    "        self.TCCS_name = TCCS_name\n",
    "        self.TCCP_name = TCCP_name\n",
    "        self.TARGET_name = TARGET_name\n",
    "        self.TCLA_name = 'tcla.a5l3.b2'\n",
    "        self.TCCS_loc_b1 =  6773.7\n",
    "        self.TCCP_loc_b1 = 6653.3\n",
    "        self.TCCS_loc = None\n",
    "        self.TCCP_loc = None\n",
    "        self.TARGET_loc = None\n",
    "        self.TCLA_loc = None\n",
    "        \n",
    "        self.beam = beam\n",
    "        self.plane = plane\n",
    "        self.job_num_part = job_num_part\n",
    "        self.job_num_turns = job_num_turns\n",
    "        self.engine = engine\n",
    "        self.plane = plane\n",
    "        self.sigma_TCCS = sigma_TCCS\n",
    "        self.sigma_TCCP = sigma_TCCP\n",
    "        self.coll_dict = None\n",
    "        self.end_s = None\n",
    "        self.line = None\n",
    "        \n",
    "    def load_colldb_new(self, filename):\n",
    "        with open(filename, \"r\") as infile:\n",
    "            coll_data_string = \"\"\n",
    "            family_settings = {}\n",
    "            family_types = {}\n",
    "            onesided = {}\n",
    "            tilted = {}\n",
    "            bend = {}\n",
    "            xdim = {}\n",
    "            ydim = {}\n",
    "\n",
    "            for l_no, line in enumerate(infile):\n",
    "                if line.startswith(\"#\"):\n",
    "                    continue  # Comment\n",
    "                if len(line.strip()) == 0:\n",
    "                    continue  # Empty line\n",
    "                sline = line.split()\n",
    "                if len(sline) < 6 or sline[0].lower() == \"crystal\" or sline[0].lower() == \"target\":\n",
    "                    if sline[0].lower() == \"nsig_fam\":\n",
    "                        family_settings[sline[1]] = sline[2]\n",
    "                        family_types[sline[1]] = sline[3]\n",
    "                    elif sline[0].lower() == \"onesided\":\n",
    "                        onesided[sline[1]] = int(sline[2])\n",
    "                    elif sline[0].lower() == \"tilted\":\n",
    "                        tilted[sline[1]] = [float(sline[2]), float(sline[3])]\n",
    "                    elif sline[0].lower() == \"crystal\":\n",
    "                        bend[sline[1]] = float(sline[2])\n",
    "                        xdim[sline[1]] = float(sline[3])\n",
    "                        ydim[sline[1]] = float(sline[4])\n",
    "                    elif sline[0].lower() == \"target\":\n",
    "                        xdim[sline[1]] = float(sline[2])\n",
    "                        ydim[sline[1]] = float(sline[3])\n",
    "                    elif sline[0].lower() == \"settings\":\n",
    "                        pass  # Acknowledge and ignore this line\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown setting {line}\")\n",
    "                else:\n",
    "                    coll_data_string += line\n",
    "\n",
    "        names = [\"name\", \"opening\", \"material\", \"length\", \"angle\", \"offset\"]\n",
    "\n",
    "        df = pd.read_csv(io.StringIO(coll_data_string), delim_whitespace=True,\n",
    "                         index_col=False, skip_blank_lines=True, names=names)\n",
    "\n",
    "        df[\"angle\"] = df[\"angle\"] \n",
    "        df[\"name\"] = df[\"name\"].str.lower() # Make the names lowercase for easy processing\n",
    "        df[\"nsigma\"] = df[\"opening\"].apply(lambda s: float(family_settings.get(s, s)))\n",
    "        df[\"type\"] = df[\"opening\"].apply(lambda s: family_types.get(s, \"UNKNOWN\"))\n",
    "        df[\"side\"] = df[\"name\"].apply(lambda s: onesided.get(s, 0))\n",
    "        df[\"bend\"] = df[\"name\"].apply(lambda s: bend.get(s, 0))\n",
    "        df[\"xdim\"] = df[\"name\"].apply(lambda s: xdim.get(s, 0))\n",
    "        df[\"ydim\"] = df[\"name\"].apply(lambda s: ydim.get(s, 0))\n",
    "        df[\"tilt_left\"] = df[\"name\"].apply(lambda s: np.deg2rad(tilted.get(s, [0, 0])[0]))\n",
    "        df[\"tilt_right\"] = df[\"name\"].apply(lambda s: np.deg2rad(tilted.get(s, [0, 0])[1]))\n",
    "        df.rename(columns={\"opening\": \"family\"}, inplace=True)\n",
    "        df = df.set_index(\"name\").T\n",
    "\n",
    "        # Ensure the collimators marked as one-sided or tilted are actually defined\n",
    "        defined_set = set(df.columns) # The data fram was transposed so columns are names\n",
    "        onesided_set = set(onesided.keys())\n",
    "        tilted_set = set(tilted.keys())\n",
    "        if not onesided_set.issubset(defined_set):\n",
    "            different = onesided_set - defined_set\n",
    "            raise SystemExit('One-sided collimators not defined: {}'.format(\", \".join(different)))\n",
    "        if not tilted_set.issubset(defined_set):\n",
    "            different = tilted_set - defined_set\n",
    "            raise SystemExit('Tilted collimators not defined: {}'.format(\",\".join(different)))\n",
    "        return df.T\n",
    "\n",
    "\n",
    "    def find_axis_intercepts(self, x_coords, y_coords):\n",
    "        x_intercepts = []\n",
    "        y_intercepts = []\n",
    "\n",
    "        for i in range(len(x_coords)):\n",
    "            x1, y1 = x_coords[i], y_coords[i]\n",
    "            x2, y2 = x_coords[(i + 1) % len(x_coords)], y_coords[(i + 1) % len(y_coords)]\n",
    "\n",
    "            if x1 == x2:\n",
    "            # Vertical line, no y-intercept\n",
    "                y_intercept = 0.0 if x1 == x2 == 0.0 else None\n",
    "            else:\n",
    "                slope = (y2 - y1) / (x2 - x1)\n",
    "                y_intercept = y1 - (slope * x1)\n",
    "\n",
    "            if y1 == y2:\n",
    "            # Horizontal line, no x-intercept\n",
    "                x_intercept = 0.0 if y1 == y2 == 0.0 else None\n",
    "            else:\n",
    "                slope = (x2 - x1) / (y2 - y1)\n",
    "                x_intercept = x1 - (slope * y1)\n",
    "\n",
    "            # Check if the x-intercept is within the range of x1 and x2\n",
    "            if x_intercept is not None and (x1 <= x_intercept <= x2 or x2 <= x_intercept <= x1):\n",
    "                x_intercepts.append(x_intercept)\n",
    "\n",
    "            # Check if the y-intercept is within the range of y1 and y2\n",
    "            if y_intercept is not None and (y1 <= y_intercept <= y2 or y2 <= y_intercept <= y1):\n",
    "                y_intercepts.append(y_intercept)\n",
    "\n",
    "        return x_intercepts, y_intercepts\n",
    "\n",
    "\n",
    "\n",
    "    def find_bad_offset_apertures(self, line):\n",
    "        aperture_offsets = {}\n",
    "        for name, element in line.element_dict.items():\n",
    "            if 'offset' in name and element.__class__.__name__.startswith('XYShift'):\n",
    "                aper_name = name.split('_offset')[0]\n",
    "                aperture_offsets[aper_name] = (element.dx, element.dy)\n",
    "\n",
    "        bad_apers = {}\n",
    "        for ap_name, offset in aperture_offsets.items():\n",
    "            aperture_el = line.element_dict[ap_name]\n",
    "\n",
    "            cname= aperture_el.__class__.__name__\n",
    "            ap_dict = aperture_el.to_dict()\n",
    "\n",
    "            if cname == 'LimitEllipse':\n",
    "                x_min = -ap_dict['a']\n",
    "                x_max = ap_dict['a']\n",
    "                y_min = -ap_dict['b']\n",
    "                y_max = ap_dict['b']\n",
    "            elif cname == 'LimitRect':\n",
    "                x_min = ap_dict['min_x']\n",
    "                x_max = ap_dict['max_x']\n",
    "                y_min = ap_dict['min_y']\n",
    "                y_max = ap_dict['max_y']\n",
    "            elif cname == 'LimitRectEllipse':\n",
    "                x_min = -ap_dict['max_x']\n",
    "                x_max = ap_dict['max_x']\n",
    "                y_min = -ap_dict['max_y']\n",
    "                y_max = ap_dict['max_y']\n",
    "            elif cname == 'LimitRacetrack':\n",
    "                x_min = ap_dict['min_x']\n",
    "                x_max = ap_dict['max_x']\n",
    "                y_min = ap_dict['min_y']\n",
    "                y_max = ap_dict['max_y']\n",
    "            elif cname == 'LimitPolygon':\n",
    "                x_intercepts, y_intercepts = self.find_axis_intercepts(ap_dict['x_vertices'],\n",
    "                                                                ap_dict['y_vertices'])\n",
    "                x_min = min(x_intercepts)\n",
    "                x_max = max(x_intercepts)\n",
    "                y_min = min(y_intercepts)\n",
    "                y_max = max(y_intercepts)\n",
    "\n",
    "            tolerance = 5e-3\n",
    "            \"\"\"if (x_max - offset[0] < tolerance \n",
    "                or -x_min + offset[0] < tolerance \n",
    "                or y_max - offset[1] < tolerance \n",
    "                or -y_min + offset[1] < tolerance):\"\"\"\n",
    "            if (offset[0] -x_max > tolerance \n",
    "                or  -offset[0] + x_min > tolerance \n",
    "                or  offset[1] - y_max > tolerance \n",
    "                or  -offset[1] + y_min > tolerance ):\n",
    "                    bad_apers[ap_name] = (x_min, x_max, y_min, y_max, offset[0], offset[1])\n",
    "\n",
    "        return bad_apers\n",
    "\n",
    "        \n",
    "    def load_line(self):\n",
    "        \n",
    "        #TTCS_align_angle_step = run_dict['TTCS_align_angle_step']\n",
    "\n",
    "        #mode = run_dict['mode']\n",
    "        #print('\\nMode: ', mode, '\\n')\n",
    "\n",
    "        print('Input files:\\n', self.line_file, '\\n', self.coll_file, '\\n')\n",
    "\n",
    "        if self.coll_file.endswith('.yaml'):\n",
    "            with open(self.coll_file, 'r') as stream:\n",
    "                coll_dict = yaml.safe_load(stream)['collimators'][f'b{self.beam}']\n",
    "        if self.coll_file.endswith('.data'):\n",
    "            coll_dict = self.load_colldb_new(self.coll_file).to_dict('index')\n",
    "\n",
    "        context = xo.ContextCpu(omp_num_threads='auto')\n",
    "        self.coll_dict = coll_dict\n",
    "        \n",
    "        # Load Line in Xtrack\n",
    "        line = xt.Line.from_json(self.line_file)\n",
    "        self.end_s = line.get_length()\n",
    "        \n",
    "        self.TCCS_loc = self.end_s - self.TCCS_loc_b1 #6775\n",
    "        self.TCCP_loc = self.end_s - self.TCCP_loc_b1 #6655\n",
    "        self.TARGET_loc = self.end_s - (self.TCCP_loc_b1 + coll_dict[self.TCCP_name][\"length\"]/2 + coll_dict[self.TARGET_name][\"length\"]/2)\n",
    "        self.TCLA_loc = line.get_s_position()[line.element_names.index(self.TCLA_name)]\n",
    "\n",
    "        line.insert_element(at_s=self.TCCS_loc, element=xt.Marker(), name=self.TCCS_name)\n",
    "        line.insert_element(at_s=self.TCCS_loc, element=xt.LimitEllipse(a_squ=0.0016, b_squ=0.0016, a_b_squ=2.56e-06), name=self.TCCS_name+'_aper')\n",
    "        line.insert_element(at_s=self.TARGET_loc, element=xt.Marker(), name=self.TARGET_name)\n",
    "        line.insert_element(at_s=self.TARGET_loc, element=xt.LimitEllipse(a_squ=0.0016, b_squ=0.0016, a_b_squ=2.56e-06), name=self.TARGET_name+'_aper')\n",
    "        line.insert_element(at_s=self.TCCP_loc, element=xt.Marker(), name=self.TCCP_name)\n",
    "        line.insert_element(at_s=self.TCCP_loc, element=xt.LimitEllipse(a_squ=0.0016, b_squ=0.0016, a_b_squ=2.56e-06), name=self.TCCP_name+'_aper')\n",
    "\n",
    "        TCCS_monitor = xt.ParticlesMonitor(num_particles=self.job_num_part, start_at_turn=0, stop_at_turn=self.job_num_turns)\n",
    "        TARGET_monitor = xt.ParticlesMonitor(num_particles=self.job_num_part, start_at_turn=0, stop_at_turn=self.job_num_turns)\n",
    "        dx = 1e-11\n",
    "        line.insert_element(at_s = self.TCCS_loc - coll_dict[self.TCCS_name][\"length\"]/2 - dx, element=TCCS_monitor, name='TCCS_monitor')\n",
    "        line.insert_element(at_s = self.TARGET_loc - coll_dict[self.TARGET_name][\"length\"]/2 - dx, element=TARGET_monitor, name='TARGET_monitor')\n",
    "\n",
    "\n",
    "        bad_aper = self.find_bad_offset_apertures(line)\n",
    "        print('Bad apertures : ', bad_aper)\n",
    "        print('Replace bad apertures with Marker')\n",
    "        for name in bad_aper.keys():\n",
    "            line.element_dict[name] = xt.Marker()\n",
    "            print(name, line.get_s_position(name), line.element_dict[name])\n",
    "\n",
    "        # Aperture model check\n",
    "        print('\\nAperture model check on imported model:')\n",
    "        df_imported = line.check_aperture()\n",
    "        assert not np.any(df_imported.has_aperture_problem)\n",
    "\n",
    "\n",
    "        # Initialise collmanager\n",
    "        if self.coll_file.endswith('.yaml'):\n",
    "            coll_manager = xc.CollimatorManager.from_yaml(self.coll_file, line=line, beam=self.beam, _context=context, ignore_crystals=False)\n",
    "        elif self.coll_file.endswith('.data'):\n",
    "            coll_manager = xc.CollimatorManager.from_SixTrack(self.coll_file, line=line, _context=context, ignore_crystals=False, nemitt_x = 2.5e-6,  nemitt_y = 2.5e-6)\n",
    "            # switch on cavities\n",
    "            speed = line.particle_ref._xobject.beta0[0]*scipy.constants.c\n",
    "            harmonic_number = 35640\n",
    "            voltage = 12e6/len(line.get_elements_of_type(xt.Cavity)[1])\n",
    "            frequency = harmonic_number * speed /line.get_length()\n",
    "            for side in ['l', 'r']:\n",
    "                for cell in ['a','b','c','d']:\n",
    "                    line[f'acsca.{cell}5{side}4.b2'].voltage = voltage\n",
    "                    line[f'acsca.{cell}5{side}4.b2'].frequency = frequency\n",
    "\n",
    "        # Install collimators into line\n",
    "        if self.engine == 'everest':\n",
    "            coll_names = coll_manager.collimator_names\n",
    "            black_absorbers = [self.TARGET_name,]\n",
    "\n",
    "            everest_colls = [name for name in coll_names if name not in black_absorbers]\n",
    "            coll_manager.install_everest_collimators(names=everest_colls,verbose=True)\n",
    "            coll_manager.install_black_absorbers(names = black_absorbers, verbose=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scattering engine {self.engine}!\")\n",
    "        \n",
    "        # Aperture model check\n",
    "        print('\\nAperture model check after introducing collimators:')\n",
    "        df_with_coll = line.check_aperture()\n",
    "        assert not np.any(df_with_coll.has_aperture_problem)\n",
    "      \n",
    "        # Build the tracker\n",
    "        coll_manager.build_tracker()\n",
    "\n",
    "        # Set the collimator openings based on the colldb,\n",
    "        # or manually override with the option gaps={collname: gap}\n",
    "        coll_manager.set_openings()\n",
    "\n",
    "        # Aperture model check\n",
    "        print('\\nAperture model check after introducing collimators:')\n",
    "        df_with_coll = line.check_aperture()\n",
    "        assert not np.any(df_with_coll.has_aperture_problem)\n",
    "        \n",
    "        self.line = line\n",
    "            \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13a9b300",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1425/1055767407.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m line_test = LineData(run='HL', line_file_name = 'input_files/HL_IR7_rematched/b4_sequence_patched.json', \n\u001b[0m\u001b[1;32m      2\u001b[0m                      coll_file_name = 'input_files/CollDB_HL_tight_b4.data')\n",
      "\u001b[0;32m/tmp/ipykernel_1425/3563897963.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, run, line_file_name, coll_file_name, TCCS_name, TCCP_name, TARGET_name, TCCS_loc_b1, TCCP_loc_b1, beam, plane, engine, sigma_TCCS, sigma_TCCP, job_num_part, job_num_turns)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{os.environ.get('HOME_TWOCRYST')}/{line_file_name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoll_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{os.environ.get('HOME_TWOCRYST')}/{coll_file_name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "line_test = LineData(run='HL', line_file_name = 'input_files/HL_IR7_rematched/b4_sequence_patched.json', \n",
    "                     coll_file_name = 'input_files/CollDB_HL_tight_b4.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f556f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_test.load_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "6bb094ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'family': 'tccs3',\n",
       " 'material': 'Si',\n",
       " 'length': 0.004,\n",
       " 'angle': 90.0,\n",
       " 'offset': 0.0,\n",
       " 'nsigma': 8.7,\n",
       " 'type': 'SPECIAL',\n",
       " 'side': 1,\n",
       " 'bend': 80.0,\n",
       " 'xdim': 0.002,\n",
       " 'ydim': 0.035,\n",
       " 'tilt_left': 0.0,\n",
       " 'tilt_right': 0.0}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_test.coll_dict[ 'tccs.5r3.b2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "53215478",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1613574649.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndentationError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_104a_swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/IPython/core/compilerop.py\u001b[0m in \u001b[0;36mast_parse\u001b[0;34m(self, source, filename, symbol)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mArguments\u001b[0m \u001b[0mare\u001b[0m \u001b[0mexactly\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstandard\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         and are passed to the built-in compile function.\"\"\"\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mPyCF_ONLY_AST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_compiler_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndentationError\u001b[0m: expected an indented block (1613574649.py, line 1)"
     ]
    }
   ],
   "source": [
    " config_file = sys.argv[1]\n",
    "    \n",
    "    with open(config_file, 'r') as stream:\n",
    "        config_dict = yaml.safe_load(stream)\n",
    "\n",
    "    # Configure run parameters\n",
    "    run_dict = config_dict['run']\n",
    "\n",
    "    beam          = run_dict['beam']\n",
    "    plane         = run_dict['plane']\n",
    "\n",
    "    num_turns     = run_dict['turns']\n",
    "    num_particles = run_dict['nparticles']\n",
    "    engine        = run_dict['engine']\n",
    "\n",
    "    TTCS_align_angle_step = run_dict['TTCS_align_angle_step']\n",
    "\n",
    "    mode = run_dict['mode']\n",
    "    print('\\nMode: ', mode, '\\n')\n",
    "\n",
    "\n",
    "    # Setup input files\n",
    "    file_dict = config_dict['input_files']\n",
    "\n",
    "    coll_file = os.path.expandvars(file_dict['collimators'])\n",
    "    line_file = os.path.expandvars(file_dict[f'line_b{beam}'])\n",
    "    \n",
    "    print('Input files:\\n', line_file, '\\n', coll_file, '\\n')\n",
    "\n",
    "    if coll_file.endswith('.yaml'):\n",
    "        with open(coll_file, 'r') as stream:\n",
    "            coll_dict = yaml.safe_load(stream)['collimators']['b'+config_dict['run']['beam']]\n",
    "    if coll_file.endswith('.data'):\n",
    "        coll_dict = load_colldb_new(coll_file).to_dict('index')\n",
    "\n",
    "    context = xo.ContextCpu(omp_num_threads='auto')\n",
    "\n",
    "\n",
    "    # Define output path\n",
    "    path_out = Path.cwd() / 'Outputdata'\n",
    "\n",
    "    if not path_out.exists():\n",
    "        os.makedirs(path_out)\n",
    "\n",
    "\n",
    "    # Load from json\n",
    "    line = xt.Line.from_json(line_file)\n",
    "\n",
    "    end_s = line.get_length()\n",
    "\n",
    "    TCCS_name = 'tccs.5r3.b2'\n",
    "    TCCP_name = 'tccp.4l3.b2'\n",
    "    TARGET_name = 'target.4l3.b2'\n",
    "    TCLA_name = 'tcla.a5l3.b2'\n",
    "\n",
    "    TCCS_loc = end_s - 6773.7 #6775\n",
    "    TCCP_loc = end_s - 6653.3 #6655\n",
    "    TARGET_loc = end_s - (6653.3 + coll_dict[TCCP_name][\"length\"]/2 + coll_dict[TARGET_name][\"length\"]/2)\n",
    "    TCLA_loc = line.get_s_position()[line.element_names.index(TCLA_name)]\n",
    "\n",
    "\n",
    "    line.insert_element(at_s=TCCS_loc, element=xt.Marker(), name='tccs.5r3.b2')\n",
    "    line.insert_element(at_s=TCCS_loc, element=xt.LimitEllipse(a_squ=0.0016, b_squ=0.0016, a_b_squ=2.56e-06), name='tccs.5r3.b2_aper')\n",
    "    line.insert_element(at_s=TCCP_loc, element=xt.Marker(), name='tccp.4l3.b2')\n",
    "    line.insert_element(at_s=TCCP_loc, element=xt.LimitEllipse(a_squ=0.0016, b_squ=0.0016, a_b_squ=2.56e-06), name='tccp.4l3.b2_aper')\n",
    "    line.insert_element(at_s=TARGET_loc, element=xt.Marker(), name='target.4l3.b2')\n",
    "    line.insert_element(at_s=TARGET_loc, element=xt.LimitEllipse(a_squ=0.0016, b_squ=0.0016, a_b_squ=2.56e-06), name='target.4l3.b2_aper')\n",
    "\n",
    "\n",
    "    TCCS_monitor = xt.ParticlesMonitor(num_particles=num_particles, start_at_turn=0, stop_at_turn=num_turns)\n",
    "    TARGET_monitor = xt.ParticlesMonitor(num_particles=num_particles, start_at_turn=0, stop_at_turn=num_turns)\n",
    "    dx = 1e-11\n",
    "    line.insert_element(at_s = TCCS_loc - coll_dict[TCCS_name][\"length\"]/2 - dx, element=TCCS_monitor, name='TCCS_monitor')\n",
    "    line.insert_element(at_s = TARGET_loc - coll_dict[TARGET_name][\"length\"]/2 - dx, element=TARGET_monitor, name='TARGET_monitor')\n",
    "\n",
    "\n",
    "    bad_aper = find_bad_offset_apertures(line)\n",
    "    print('Bad apertures : ', bad_aper)\n",
    "    print('Replace bad apertures with Marker')\n",
    "    for name in bad_aper.keys():\n",
    "        line.element_dict[name] = xt.Marker()\n",
    "        print(name, line.get_s_position(name), line.element_dict[name])\n",
    "\n",
    "    # Aperture model check\n",
    "    print('\\nAperture model check on imported model:')\n",
    "    df_imported = line.check_aperture()\n",
    "    assert not np.any(df_imported.has_aperture_problem)\n",
    "\n",
    "\n",
    "    # Initialise collmanager\n",
    "    if coll_file.endswith('.yaml'):\n",
    "        coll_manager = xc.CollimatorManager.from_yaml(coll_file, line=line, beam=beam, _context=context, ignore_crystals=False)\n",
    "    elif coll_file.endswith('.data'):\n",
    "        coll_manager = xc.CollimatorManager.from_SixTrack(coll_file, line=line, _context=context, ignore_crystals=False, nemitt_x = 2.5e-6,  nemitt_y = 2.5e-6)\n",
    "        # switch on cavities\n",
    "        speed = line.particle_ref._xobject.beta0[0]*scipy.constants.c\n",
    "        harmonic_number = 35640\n",
    "        voltage = 12e6/len(line.get_elements_of_type(xt.Cavity)[1])\n",
    "        frequency = harmonic_number * speed /line.get_length()\n",
    "        for side in ['l', 'r']:\n",
    "            for cell in ['a','b','c','d']:\n",
    "                line[f'acsca.{cell}5{side}4.b2'].voltage = voltage\n",
    "                line[f'acsca.{cell}5{side}4.b2'].frequency = frequency\n",
    "\n",
    "    # Install collimators into line\n",
    "    if engine == 'everest':\n",
    "        coll_names = coll_manager.collimator_names\n",
    "\n",
    "        if mode == 'cry_black_absorbers':\n",
    "            black_absorbers = ['target.4l3.b2', 'tccs.5r3.b2']\n",
    "        elif mode == 'angular_scan' or mode == 'target_absorber': \n",
    "            black_absorbers = ['target.4l3.b2',]\n",
    "        else: \n",
    "            black_absorbers = []\n",
    "\n",
    "        everest_colls = [name for name in coll_names if name not in black_absorbers]\n",
    "        coll_manager.install_everest_collimators(names=everest_colls,verbose=True)\n",
    "        coll_manager.install_black_absorbers(names = black_absorbers, verbose=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scattering engine {engine}!\")\n",
    "\n",
    "\n",
    "    # Aperture model check\n",
    "    print('\\nAperture model check after introducing collimators:')\n",
    "    df_with_coll = line.check_aperture()\n",
    "    assert not np.any(df_with_coll.has_aperture_problem)\n",
    "\n",
    "        \n",
    "    # Build the tracker\n",
    "    coll_manager.build_tracker()\n",
    "\n",
    "\n",
    "    # Set the collimator openings based on the colldb,\n",
    "    # or manually override with the option gaps={collname: gap}\n",
    "    coll_manager.set_openings()\n",
    "\n",
    "\n",
    "    if mode == 'angular_scan':\n",
    "        print(\"\\nTCCS aligned to beam: \", line[TCCS_name].align_angle)\n",
    "        #line[TTCS_name].align_angle = TTCS_align_angle_step\n",
    "\n",
    "        line[TCCS_name].align_angle = line[TCCS_name].align_angle + TTCS_align_angle_step\n",
    "        print(\"TCCS align angle incremented by step: \", line[TCCS_name].align_angle)\n",
    "\n",
    "\n",
    "    # Aperture model check\n",
    "    print('\\nAperture model check after introducing collimators:')\n",
    "    df_with_coll = line.check_aperture()\n",
    "    assert not np.any(df_with_coll.has_aperture_problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1a95bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_file_path(prefix_name, path = \"/eos/home-i04/c/cmaccani/xsuite_sim/two_cryst_sim/Condor/\"):\n",
    "\n",
    "    test_list = [path + i for i in os.listdir(path) if prefix_name in i]\n",
    "    for test_name in test_list:\n",
    "\n",
    "        n_jobs = int(subprocess.check_output(\"find \"+ test_name +\" -maxdepth 1 -mindepth 1 -type d | grep Job. | wc -l\", shell=True))\n",
    "\n",
    "        part_dfs =[]\n",
    "\n",
    "        for i in range(n_jobs):\n",
    "            file_path_part = (subprocess.check_output(\"echo \" + test_name + '/Job.' + str(i) + '/Outputdata/particle*.h5', shell=True)).decode('ascii').strip()\n",
    "            if os.path.exists(file_path_part):\n",
    "                part_dfs.append(file_path_part)\n",
    "\n",
    "        n_jobs_verify  = len(part_dfs)\n",
    "        if n_jobs != n_jobs_verify:\n",
    "            print(\"!!! Succesful Jobs: \", n_jobs_verify_TARGET, '/', n_jobs, ' in file: ', test_name)\n",
    "    return part_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5880441",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"TEST_IR3_IR7rem_TCCS_7.2__target_absorber_20231222-1829\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edc4dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = retreive_file_path(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "351c1601",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleAnalysis():\n",
    "\n",
    "    def __init__(self, element_type, n_sigma, length, xdim, ydim, jaw_L, bend=None, align_angle = None, element_id= None, \n",
    "                 beam = 2, plane = 'V',\n",
    "                 pot_crit = 21.34, #16 #eV Silicon\n",
    "                 en_crit = 5.7e9 / 1e-2 #eV/m Silicon    #dp_Si = 1.92e-10 #m\n",
    "                 ):\n",
    "\n",
    "        self.beam = beam\n",
    "        self.plane = plane\n",
    "        self.element_type = element_type\n",
    "        self.n_sigma = n_sigma\n",
    "        self.length = length\n",
    "        self.xdim = xdim\n",
    "        self.ydim = ydim\n",
    "        self.align_angle = align_angle\n",
    "        self.jaw_L = jaw_L\n",
    "        self.en_crit = en_crit\n",
    "        self.pot_crit = pot_crit\n",
    "        self.element_id = element_id\n",
    "        self.p0c = None\n",
    "        self.npart = None\n",
    "        self.data = None\n",
    "        self.impact_part = None\n",
    "        self.abs_y_low = self.jaw_L\n",
    "        self.abs_y_up = self.jaw_L + self.ydim\n",
    "        self.abs_x_low = -self.xdim/2\n",
    "        self.abs_x_up = self.ydim/2\n",
    "        \n",
    "        if element_type == 'crystal':\n",
    "            self.bend = bend \n",
    "\n",
    "    def load(self, h5_file_path, df_key = 'particles'):\n",
    "\n",
    "        if not hasattr(h5_file_path, '__iter__') or isinstance(h5_file_path, str):\n",
    "            h5_file_path = [h5_file_path]\n",
    "\n",
    "        df_particles = pd.DataFrame()\n",
    "        npart = 0\n",
    "\n",
    "        for file in h5_file_path:\n",
    "            try:\n",
    "                df_tmp = pd.read_hdf(file, key=df_key) #test_particles_B2V\n",
    "                df_tmp['particle_id'] = df_tmp['particle_id'] + npart    \n",
    "                df_particles = pd.concat([df_particles, df_tmp])\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: File not found at {file}\")\n",
    "                continue\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"Error: The HDF5 file at {file} is empty.\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error: An unexpected error occurred - {e}\")\n",
    "                continue\n",
    "            npart = npart + len(df_tmp)\n",
    "        #check if there are duplicated     \n",
    "        data = df_particles['particle_id']\n",
    "        duplicated_mask = data.duplicated()\n",
    "        if len(data[duplicated_mask]) != 0:\n",
    "            print(f'There are repeated particle ids: {data[duplicated_mask]}')\n",
    "            \n",
    "        self.npart = npart       \n",
    "        self.data = df_particles\n",
    "            \n",
    "        p0c = None\n",
    "        if len(test.data['p0c'].unique()) == 1:\n",
    "            p0c = test.data['p0c'].unique()[0]\n",
    "        else:\n",
    "            print('There are particles at different energies')            \n",
    "        self.p0c = p0c\n",
    "        \n",
    "        if self.element_type == 'crystal':\n",
    "            xp_crit0 = np.sqrt(2.0*self.pot_crit/self.p0c)\n",
    "            Rcrit = self.p0c/self.en_crit\n",
    "            self.xp_crit = xp_crit0*(1-Rcrit/self.bend)\n",
    "            if(self.xp_crit < 0):\n",
    "                print(\"!!!!!!!!!!!!! \\nERROR: particles at\",self.p0c, f\"eV cannot be channeled if bending is {self.bend} m \\n!!!!!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "19f08c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/eos/home-i04/c/cmaccani/xsuite_sim/two_cryst_sim/Condor/TEST_IR3_IR7rem_TCCS_7.2__target_absorber_20231222-1829/Job.0/Outputdata/particles_B2V.h5',\n",
       " '/eos/home-i04/c/cmaccani/xsuite_sim/two_cryst_sim/Condor/TEST_IR3_IR7rem_TCCS_7.2__target_absorber_20231222-1829/Job.1/Outputdata/particles_B2V.h5',\n",
       " '/eos/home-i04/c/cmaccani/xsuite_sim/two_cryst_sim/Condor/TEST_IR3_IR7rem_TCCS_7.2__target_absorber_20231222-1829/Job.2/Outputdata/particles_B2V.h5']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c89e7122",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ParticleAnalysis(element_type='crystal', n_sigma=10, length=0.07, ydim=0.002, xdim=0.008, bend=10.0, jaw_L=0.0036950088531626946)\n",
    "#test = ParticleAnalysis(element_type='crystal', n_sigma=5.0, length=0.004, ydim=0.002, xdim=0.035, bend=80.0, align_angle=-1.1763616021881982e-05, jaw_L=0.0016912979598174786)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "44f5d59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!! \n",
      "ERROR: particles at 7000000000000.0 eV cannot be channeled if bending is 10.0 m \n",
      "!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "test.load(test_path[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5acaaca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299999"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test.data['particle_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e6a584d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7000000000000.0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.p0c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "89d2c8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7000000000000.0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(test.data['p0c'].unique()) == 1:\n",
    "    p0c = test.data['p0c'].unique()[0]\n",
    "else:\n",
    "    print('There are particles at different energies')\n",
    "p0c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fefa4d8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_565/1606260292.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'particle_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "a = max(test.data['particle_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a1475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf0500c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1381b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27352fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "   def plot_alive(self):\n",
    "        alive_part = []\n",
    "        for turn in range(self.nturns):\n",
    "            alive_part.append(self.data.where(self.data.loc[ 'state', :, turn] > 0, drop = True).shape[1])\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 5))\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.plot(self.data.coords['turn'], alive_part)\n",
    "        ax.set_xlabel('turn')\n",
    "        ax.set_ylabel('Number of particles')\n",
    "\n",
    "\n",
    "    def plot_distributions(self, x, y, px, py, **kwargs):\n",
    "\n",
    "        fig1 = plt.figure( figsize=(22, 10))\n",
    "        ax1 = fig1.add_subplot(2,3,1)\n",
    "        ax1.hist(x, bins=100)\n",
    "        ax1.set_xlabel('x [mm]')\n",
    "        ax1.set_ylabel(\"\")\n",
    "        ax1.set_xticks(ticks=plt.xticks()[0], labels=[f\"{x_tick*1e3:.{2}f}\" for x_tick in plt.xticks()[0]])\n",
    "\n",
    "        ax2 = fig1.add_subplot(2,3,2)\n",
    "        ax2.hist(y, bins=100) \n",
    "        ax2.set_xlabel('y [mm]')\n",
    "        ax2.set_ylabel('')\n",
    "        \n",
    "        ax2.set_xticks(ticks=plt.xticks()[0], labels=[f\"{x_tick*1e3:.{2}f}\" for x_tick in plt.xticks()[0]])\n",
    "        ax2.set_title(f'Total particles: {len(x)}')\n",
    "\n",
    "        ax3 = fig1.add_subplot(2,3,3)\n",
    "        h = ax3.hist2d(x, y, bins=100, norm=matplotlib.colors.LogNorm())#,  vmin = 1, vmax = 1e6, range = ([-40e-6, 40e-6], [-40e-6,40e-6])) \n",
    "        ax3.set_xlabel(r'x [mm]')\n",
    "        #ax3.set_ylim(0,0.008)\n",
    "        ax3.set_ylabel(r'y [mm]')\n",
    "        ax3.set_xticks(ticks=plt.xticks()[0], labels=[f\"{x_tick*1e3:.{2}f}\" for x_tick in plt.xticks()[0]])\n",
    "        ax3.set_yticks(ticks=plt.yticks()[0], labels=[f\"{x_tick*1e3:.{2}f}\" for x_tick in plt.yticks()[0]])\n",
    "        ax3_tw = ax3.twinx()\n",
    "        sigma_abs = self.jaw_L/self.n_sigma\n",
    "        ax3_tw.set_ylim(ax3.get_ylim()[0]/sigma_abs, ax3.get_ylim()[1]/sigma_abs)\n",
    "        yticks = np.arange(np.ceil(ax3_tw.get_ylim()[0]), np.floor(ax3_tw.get_ylim()[1]) + 1, 2)\n",
    "        ax3_tw.set_yticks(yticks)\n",
    "        ax3_tw.set_ylabel(r' n $\\sigma$')\n",
    "\n",
    "        if \"sigma\" in kwargs and kwargs['sigma'] == True:\n",
    "            ax3_tw.axhline(self.n_sigma, color = 'r', linestyle = '--')\n",
    "            #ax3_tw.text( 0,0, r'TCP $\\sigma$') #max(ax3.get_xticks())-1.5e-3, 4,\n",
    "\n",
    "        axins = inset_axes(ax3, height=\"100%\",  width=\"5%\", loc='right', borderpad=-6 )\n",
    "        fig1.colorbar(h[3], cax=axins, orientation='vertical', label='Count (log scale)')\n",
    "        ax3.grid(linestyle=':')\n",
    "\n",
    "        ax12 = fig1.add_subplot(2,3,4)\n",
    "        ax12.hist(px, bins=100)\n",
    "        ax12.set_xlabel(r'px [$\\mu$rad]')\n",
    "        ax12.set_ylabel(\"\")\n",
    "        \n",
    "        ax12.set_xticks(ticks=plt.xticks()[0], labels=[f\"{x_tick*1e6:.{1}f}\" for x_tick in plt.xticks()[0]])\n",
    "\n",
    "\n",
    "        ax22 = fig1.add_subplot(2,3,5)\n",
    "        ax22.hist(py, bins=100) \n",
    "        ax22.set_xlabel(r'py [$\\mu$rad]')\n",
    "        ax22.set_ylabel('')\n",
    "        ax22.set_xticks(ticks=plt.xticks()[0], labels=[f\"{x_tick*1e6:.{1}f}\" for x_tick in plt.xticks()[0]])\n",
    "        if self.element_type == 'crystal' and \"xpcrit\" in kwargs and kwargs['xpcrit'] == True:   \n",
    "            mean_angle = None\n",
    "            if self.align_angle is None and \"py_mean\" in kwargs:\n",
    "                mean_angle = kwargs['py_mean']\n",
    "            elif self.align_angle is not None:\n",
    "                mean_angle = self.align_angle\n",
    "            elif \"calculate_mean\" in kwargs and kwargs['calculate_mean'] == True:\n",
    "                mean_angle = self.calculate_mean_py(**kwargs)\n",
    "             \n",
    "            if mean_angle is not None: \n",
    "                ax22.axvline(mean_angle, color = 'red', linestyle = '-', alpha = 0.8)\n",
    "                if(self.xp_crit < 0):\n",
    "                    print(f\"!!!!!!!!!!!!! \\nERROR: particles at {self.p0c} cannot be channeled if bending is {self.bend} \\n!!!!!!!!!!!!!\")\n",
    "                else:\n",
    "                    ax22.axvline(mean_angle  + np.abs(self.xp_crit), color = 'red', linestyle = '--', alpha = 0.9)\n",
    "                    ax22.axvline(mean_angle - np.abs(self.xp_crit), color = 'red', linestyle = '--', alpha = 0.9)\n",
    "                    chann_mask = (py > mean_angle  - np.abs(self.xp_crit)) & (py < mean_angle  + np.abs(self.xp_crit))\n",
    "                    chann = len(py[chann_mask])\n",
    "                    ax22.set_title(f'N particle inside critical angle range: {chann}')\n",
    "\n",
    "        ax32 = fig1.add_subplot(2,3,6)\n",
    "        h2 = ax32.hist2d(px, py, bins=100, norm=matplotlib.colors.LogNorm())  #,, norm=matplotlib.colors.LogNorm() range = ([-40e-6, 40e-6], [-40e-6,40e-6])\n",
    "        ax32.set_xlabel(r'px [$\\mu$rad]')\n",
    "        ax32.set_ylabel(r'py [$\\mu$rad]')\n",
    "        ax32.set_xticks(ticks=plt.xticks()[0], labels=[f\"{x_tick*1e6:.{1}f}\" for x_tick in plt.xticks()[0]])\n",
    "        ax32.set_yticks(ticks=plt.yticks()[0], labels=[f\"{x_tick*1e6:.{1}f}\" for x_tick in plt.yticks()[0]])\n",
    "        axins_2 = inset_axes(ax32, height=\"100%\",  width=\"5%\", loc='right', borderpad=-6 )\n",
    "        fig1.colorbar(h2[3], cax=axins_2, orientation='vertical', label='Count (log scale)')\n",
    "        ax32.grid(linestyle=':')\n",
    "\n",
    "        if \"log\" in kwargs and kwargs['log'] == True:  \n",
    "            ax22.set_yscale(\"log\")\n",
    "            ax2.set_yscale(\"log\")\n",
    "            ax12.set_yscale(\"log\")\n",
    "            ax1.set_yscale(\"log\")\n",
    "\n",
    "\n",
    "        #fig1.suptitle('plot')\n",
    "        plt.show()\n",
    "        #return fig1, [ax1,ax2,ax3]\n",
    "\n",
    "\n",
    "    def plot_distributions_at_turn(self, turn, element_dimensions=False):\n",
    "        alive_arr = self.data.where(self.data.loc[ 'state', :, turn] > 0, drop = True)[:,:,turn]\n",
    "        if element_dimensions:\n",
    "            alive_arr = alive_arr.where(((alive_arr.loc[ 'x', :] > self.abs_x_low) & (alive_arr.loc[ 'x', :] < self.abs_x_up) & (alive_arr.loc[ 'y', :] > self.abs_y_low) & (alive_arr.loc[ 'y', :] < self.abs_y_up)), drop = True)\n",
    "        \n",
    "        if alive_arr.shape[1] == 0:\n",
    "            print(\"No particles!\")\n",
    "            return\n",
    "        \n",
    "        self.plot_distributions(alive_arr.loc['x', :], alive_arr.loc['y',:], alive_arr.loc['px', :], alive_arr.loc['py', :])\n",
    "\n",
    "\n",
    "    def plot_particle_history(self, property_name, part_id):\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 5))\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.plot(self.data.coords['turn'], self.data.loc[property_name, part_id])\n",
    "        ax.set_xlabel('turn')\n",
    "        ax.set_ylabel(property_name)\n",
    "\n",
    "\n",
    "    def count_upper_lower(self, turn):\n",
    "        alive_y = self.data.where(self.data.loc[ 'state', :, turn] > 0, drop = True).loc['y',:,turn]\n",
    "        print(f'Alive particle after at {self.element_type} at {turn} turn: ', len(alive_y))\n",
    "        print(f'Upper particles at {self.element_type}: ', len(alive_y.where(alive_y > 0, drop = True)))\n",
    "        print(f'Lower particles at {self.element_type}: ', len(alive_y.where(alive_y < 0, drop = True)))\n",
    "\n",
    "\n",
    "    def impacting_particles(self):\n",
    "        x, y, px, py, ids, turn_id, part_id_turn = np.empty((0,)), np.empty((0,)), np.empty((0,)), np.empty((0,)), np.empty((0,)), np.empty((0,)), np.empty((0,))\n",
    "        count_not_chann = 0\n",
    "        for turn in range(self.nturns):\n",
    "            impact_part = self.data.where(((self.data.loc[ 'x', :, turn] > self.abs_x_low) & (self.data.loc[ 'x', :, turn] < self.abs_x_up) & (self.data.loc[ 'y', :, turn] > self.abs_y_low) & (self.data.loc[ 'y', :, turn] < self.abs_y_up)), drop = True)[:,:,turn]\n",
    "            for id in part_id_turn:\n",
    "                if self.data.loc['state', id, turn] != 0:\n",
    "                    if self.element_type == 'target':         \n",
    "                        print(f\"Particle {id} not absorbed at turn {turn-1}\")\n",
    "                    elif  self.element_type == 'crystal':\n",
    "                        count_not_chann = count_not_chann + 1                    \n",
    "            \n",
    "            impact_part = impact_part.where(impact_part.loc[ 'state', :] > 0, drop = True)\n",
    "            if(impact_part.shape[1]>0):\n",
    "                part_id_turn = np.array(impact_part.coords['id'])\n",
    "                ids = np.concatenate((ids, part_id_turn))\n",
    "                x = np.concatenate((x,  np.array(impact_part.loc['x', :])))\n",
    "                y = np.concatenate((y,  np.array(impact_part.loc['y', :])))\n",
    "                px = np.concatenate((px,  np.array(impact_part.loc['px', :])))\n",
    "                py = np.concatenate((py,  np.array(impact_part.loc['py', :])))\n",
    "                turn_id = np.concatenate((turn_id, np.array([turn for i in range(len(part_id_turn))])))\n",
    "            else:\n",
    "                part_id_turn =  np.empty((0,))\n",
    "\n",
    "        # Use numpy.unique to check for repeated numbers\n",
    "        unique_parts, counts = np.unique(ids, return_counts=True)\n",
    "        repeated_indices = np.where(counts > 1)[0]\n",
    "        if len(repeated_indices) > 0 and self.element_type =='target':\n",
    "            print(f\"There are repeated numbers in the array! Particle id : {unique_parts[repeated_indices]}\\n\")\n",
    "           \n",
    "        if self.element_type == 'crystal':\n",
    "            print(\"Total particles: \", len(x))\n",
    "            print('Number of particles passed to the crystal multiple times: ', len(repeated_indices))\n",
    "            print('Number particles gone through the crystal not dead the turn after: ', count_not_chann)\n",
    "            if self.align_angle is not None:\n",
    "                if(self.xp_crit < 0):\n",
    "                    print(f\"!!!!!!!!!!!!! \\nERROR: particles at {self.p0c} cannot be channeled if bending is {self.bend} \\n!!!!!!!!!!!!!\")\n",
    "                else:\n",
    "                    chann_mask = (py > self.align_angle  - np.abs(self.xp_crit)) & (py < self.align_angle  + np.abs(self.xp_crit))\n",
    "                    chann = len(py[chann_mask])\n",
    "                    print('Number particles inside critical angle range: ', chann)\n",
    "\n",
    "        part_dict = {\"id\": ids.astype(int), \"turn_id\": turn_id.astype(int), \"x\": x, \"y\": y, \"px\":px, \"py\":py}\n",
    "\n",
    "        impact_part_df = pd.DataFrame(part_dict) \n",
    "        self.impact_part = impact_part_df  \n",
    "\n",
    "        return impact_part_df\n",
    "\n",
    "    def plot_impacting_particles(self, **kwargs):\n",
    "        if self.impact_part is None:\n",
    "            df = self.impacting_particles()\n",
    "        else:\n",
    "            df = self.impact_part \n",
    "        self.plot_distributions(df['x'], df['y'], df['px'], df['py'], **kwargs)\n",
    "    \n",
    "    def calculate_mean_py(self, **kwargs):\n",
    "        if self.impact_part is None:\n",
    "            df = self.impacting_particles()\n",
    "        else:\n",
    "            df = self.impact_part \n",
    "\n",
    "        pys = df['py']\n",
    "        if 'lower_cut' in kwargs and kwargs['lower_cut'] is not None:\n",
    "            pys = pys[pys >= kwargs['lower_cut']]\n",
    "        if 'upper_cut' in kwargs and kwargs['upper_cut'] is not None:\n",
    "            pys = pys[pys <= kwargs['upper_cut']]\n",
    "\n",
    "        # Calculate the mean of the filtered data\n",
    "        mean_value = pys.mean()\n",
    "        print('Mean py: ', mean_value)\n",
    "        return(mean_value)        "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
