{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import xobjects as xo\n",
    "import xtrack as xt\n",
    "import xpart as xp\n",
    "import xcoll as xc\n",
    "\n",
    "import xcoll_plotting as xplt\n",
    "\n",
    "\n",
    "context = xo.ContextCpu() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test line generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading line from dict.           \n"
     ]
    }
   ],
   "source": [
    "#line = xt.Line.from_json('../input_files/flat_top_b2.json')\n",
    "#line = xt.Line.from_json('TEST_v3.json')\n",
    "line = xt.Line.from_json('TEST_nooct.json')\n",
    "line.particle_ref = xp.Particles(p0c=6800e9, q0=1, mass0=xp.PROTON_MASS_EV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling ContextCpu kernels...\n",
      "Done compiling ContextCpu kernels.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TwissTable: 103384 rows, 35 cols"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.twiss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### WARNING: Files Are Different! ###\n"
     ]
    }
   ],
   "source": [
    "! cmp --silent TEST_v2.json TEST_nooct.json && echo '### SUCCESS: Files Are Identical! ###' || echo '### WARNING: Files Are Different! ###'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SUCCESS: Files Are Identical! ###\n"
     ]
    }
   ],
   "source": [
    "! cmp --silent TEST_v2.json TEST_v3.json && echo '### SUCCESS: Files Are Identical! ###' || echo '### WARNING: Files Are Different! ###'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test closed orbit after aperture correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_axis_intercepts(x_coords, y_coords):\n",
    "    x_intercepts = []\n",
    "    y_intercepts = []\n",
    "\n",
    "    for i in range(len(x_coords)):\n",
    "        x1, y1 = x_coords[i], y_coords[i]\n",
    "        x2, y2 = x_coords[(i + 1) % len(x_coords)], y_coords[(i + 1) % len(y_coords)]\n",
    "\n",
    "        if x1 == x2:\n",
    "        # Vertical line, no y-intercept\n",
    "            y_intercept = 0.0 if x1 == x2 == 0.0 else None\n",
    "        else:\n",
    "            slope = (y2 - y1) / (x2 - x1)\n",
    "            y_intercept = y1 - (slope * x1)\n",
    "\n",
    "        if y1 == y2:\n",
    "        # Horizontal line, no x-intercept\n",
    "            x_intercept = 0.0 if y1 == y2 == 0.0 else None\n",
    "        else:\n",
    "            slope = (x2 - x1) / (y2 - y1)\n",
    "            x_intercept = x1 - (slope * y1)\n",
    "\n",
    "        # Check if the x-intercept is within the range of x1 and x2\n",
    "        if x_intercept is not None and (x1 <= x_intercept <= x2 or x2 <= x_intercept <= x1):\n",
    "            x_intercepts.append(x_intercept)\n",
    "\n",
    "        # Check if the y-intercept is within the range of y1 and y2\n",
    "        if y_intercept is not None and (y1 <= y_intercept <= y2 or y2 <= y_intercept <= y1):\n",
    "            y_intercepts.append(y_intercept)\n",
    "\n",
    "    return x_intercepts, y_intercepts\n",
    "\n",
    "\n",
    "\n",
    "def find_bad_offset_apertures(line):\n",
    "    aperture_offsets = {}\n",
    "    for name, element in line.element_dict.items():\n",
    "        if 'offset' in name and element.__class__.__name__.startswith('XYShift'):\n",
    "            aper_name = name.split('_offset')[0]\n",
    "            aperture_offsets[aper_name] = (element.dx, element.dy)\n",
    "\n",
    "    bad_apers = {}\n",
    "    print('vmabc.4l2.b.b2_aper' in aperture_offsets.keys())\n",
    "    for ap_name, offset in aperture_offsets.items():\n",
    "        aperture_el = line.element_dict[ap_name]\n",
    "\n",
    "        cname= aperture_el.__class__.__name__\n",
    "        ap_dict = aperture_el.to_dict()\n",
    "\n",
    "        if cname == 'LimitEllipse':\n",
    "            x_min = -ap_dict['a']\n",
    "            x_max = ap_dict['a']\n",
    "            y_min = -ap_dict['b']\n",
    "            y_max = ap_dict['b']\n",
    "        elif cname == 'LimitRect':\n",
    "            x_min = ap_dict['min_x']\n",
    "            x_max = ap_dict['max_x']\n",
    "            y_min = ap_dict['min_y']\n",
    "            y_max = ap_dict['max_y']\n",
    "        elif cname == 'LimitRectEllipse':\n",
    "            x_min = -ap_dict['max_x']\n",
    "            x_max = ap_dict['max_x']\n",
    "            y_min = -ap_dict['max_y']\n",
    "            y_max = ap_dict['max_y']\n",
    "        elif cname == 'LimitRacetrack':\n",
    "            x_min = ap_dict['min_x']\n",
    "            x_max = ap_dict['max_x']\n",
    "            y_min = ap_dict['min_y']\n",
    "            y_max = ap_dict['max_y']\n",
    "        elif cname == 'LimitPolygon':\n",
    "            x_intercepts, y_intercepts = find_axis_intercepts(ap_dict['x_vertices'],\n",
    "                                                            ap_dict['y_vertices'])\n",
    "            x_min = min(x_intercepts)\n",
    "            x_max = max(x_intercepts)\n",
    "            y_min = min(y_intercepts)\n",
    "            y_max = max(y_intercepts)\n",
    "\n",
    "        tolerance = 5e-3\n",
    "        \"\"\"if (x_max - offset[0] < tolerance \n",
    "            or -x_min + offset[0] < tolerance \n",
    "            or y_max - offset[1] < tolerance \n",
    "            or -y_min + offset[1] < tolerance):\"\"\"\n",
    "        if (offset[0] -x_max > tolerance \n",
    "            or  -offset[0] + x_min > tolerance \n",
    "            or  offset[1] - y_max > tolerance \n",
    "            or  -offset[1] + y_min > tolerance ):\n",
    "                bad_apers[ap_name] = (x_min, x_max, y_min, y_max, offset[0], offset[1])\n",
    "\n",
    "    return bad_apers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"config_sim.yaml\"\n",
    "with open(config_file, 'r') as stream:\n",
    "    config_dict = yaml.safe_load(stream)\n",
    "\n",
    "\n",
    "\n",
    "sub_dict = config_dict['run']\n",
    "file_dict = config_dict['input_files']\n",
    "\n",
    "\n",
    "context = xo.ContextCpu(omp_num_threads='auto')\n",
    "\n",
    "# On a modern CPU, we get ~5000 particle*turns/s\n",
    "# So this script should take around half an hour\n",
    "beam          = sub_dict['beam']\n",
    "plane         = sub_dict['plane']\n",
    "\n",
    "num_turns     = sub_dict['turns']\n",
    "num_particles = sub_dict['nparticles']\n",
    "engine        = sub_dict['engine']\n",
    "\n",
    "\n",
    "path_out = Path.cwd() / 'Outputdata'\n",
    "\n",
    "if not path_out.exists():\n",
    "    os.makedirs(path_out)\n",
    "\n",
    "\n",
    "# Load from json\n",
    "line = xt.Line.from_json(file_dict[f'line_b{beam}'])\n",
    "\n",
    "\n",
    "#line.element_dict('vmabc.4l2.b2_aper').max_x=100\n",
    "\n",
    "#line.particle_ref = xp.Particles(p0c=6800e9, q0=1, mass0=xp.PROTON_MASS_EV)\n",
    "#tw=line.twiss()\n",
    "\n",
    "end_s = line.get_length()\n",
    "\n",
    "\n",
    "TCCS_loc = end_s - 6673.7 #6775\n",
    "TCCP_loc = end_s - 6653.3 #6655\n",
    "TARGET_loc = end_s - (6653.3 + 0.07/2 +0.005/2)\n",
    "\n",
    "\n",
    "line.insert_element(at_s=TCCS_loc, element=xt.Marker(), name='tccs.3.b2')\n",
    "line.insert_element(at_s=TCCS_loc, element=xt.LimitEllipse(a_squ=0.0016, b_squ=0.0016, a_b_squ=2.56e-06), name='tccs.3.b2_aper')\n",
    "line.insert_element(at_s=TCCP_loc, element=xt.Marker(), name='tccp.3.b2')\n",
    "line.insert_element(at_s=TCCP_loc, element=xt.LimitEllipse(a_squ=0.0016, b_squ=0.0016, a_b_squ=2.56e-06), name='tccp.3.b2_aper')\n",
    "line.insert_element(at_s=TARGET_loc, element=xt.Marker(), name='target.3.b2')\n",
    "line.insert_element(at_s=TARGET_loc, element=xt.LimitEllipse(a_squ=0.0016, b_squ=0.0016, a_b_squ=2.56e-06), name='target.3.b2_aper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_aper = find_bad_offset_apertures(line)\n",
    "print('!! Bad apertures : ', bad_aper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Replace bad apertures with Marker')\n",
    "for name in bad_aper.keys():\n",
    "    line.element_dict[name] = xt.Marker()\n",
    "    print(line.element_dict[name])\n",
    "\n",
    "\n",
    "for name in bad_aper.keys():\n",
    "    print(name, line.element_dict[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aperture model check\n",
    "print('\\nAperture model check on imported model:')\n",
    "df_imported = line.check_aperture()\n",
    "assert not np.any(df_imported.has_aperture_problem)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialise collmanager\n",
    "coll_manager = xc.CollimatorManager.from_yaml(file_dict['collimators'], line=line, beam=beam, _context=context, ignore_crystals=False)\n",
    "#print(coll_manager.collimator_names)\n",
    "\n",
    "# Install collimators into line\n",
    "if engine == 'everest':\n",
    "    coll_manager.install_everest_collimators(verbose=True)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown scattering engine {engine}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line[name].align_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aperture model check\n",
    "print('\\nAperture model check after introducing collimators:')\n",
    "df_with_coll = line.check_aperture()\n",
    "assert not np.any(df_with_coll.has_aperture_problem)\n",
    "\n",
    "    \n",
    "# Build the tracker\n",
    "coll_manager.build_tracker()\n",
    "\n",
    "\n",
    "# Set the collimator openings based on the colldb,\n",
    "# or manually override with the option gaps={collname: gap}\n",
    "coll_manager.set_openings()\n",
    "\n",
    "# Aperture model check\n",
    "print('\\nAperture model check after introducing collimators:')\n",
    "df_with_coll = line.check_aperture()\n",
    "#assert not np.any(df_with_coll.has_aperture_problem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test angle crystal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line[name].align_angle\n",
    "line[name].align_angle = line[name].align_angle+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Generate initial pencil distribution on horizontal collimator\n",
    "    tcp  = f\"tcp.{'c' if plane=='H' else 'd'}6{'l' if beam=='1' else 'r'}7.b{beam}\"\n",
    "    part = coll_manager.generate_pencil_on_collimator(tcp, num_particles=num_particles)\n",
    "\n",
    "\n",
    "    # Optimise the line\n",
    "    #line.optimize_for_tracking()\n",
    "    idx = line.element_names.index(tcp)\n",
    "    part.at_element = idx\n",
    "    part.start_tracking_at_element = idx\n",
    "\n",
    "\n",
    "    # Track\n",
    "    coll_manager.enable_scattering()\n",
    "    line.track(part, num_turns=num_turns, time=True)\n",
    "    coll_manager.disable_scattering()\n",
    "    print(f\"Done tracking in {line.time_last_track:.1f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = line.element_names.index('tcla.a5l3.b2')\n",
    "start = line.element_names.index('tcsg.5r3.b2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(start-400, stop+400):\n",
    "    if (line.element_names[i].endswith('b2') and type(line[i]) !=  xt.beam_elements.elements.Marker) or line.element_names[i].startswith('ip'):\n",
    "        print(line.element_names[i], '\\t\\t', end_s-line.get_s_elements()[i], '\\t\\t', line[i].__class__.__name__ )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_xsuite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
